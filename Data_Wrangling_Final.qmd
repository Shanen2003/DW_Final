---
title: "Data_Wrangling_Final.qmd"
author: "Shane Bateman"
format:
  html:
    toc: true
    toc-location: left
    self-contained: true
engine: python3
---

Dataset: https://www.kaggle.com/datasets/nazishjaveed/credit-card-application/data

Here we import all needed packages. 
```{python}
import pandas as pd
import numpy as np
from scipy.stats import pointbiserialr
from plotnine import *
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import Lasso, LassoCV
import statsmodels.formula.api as smf
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
from sklearn.metrics import (
    accuracy_score, log_loss, roc_auc_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, f1_score
)
```

Here we import the CSV of data we will be using. 
```{python}
df = pd.read_csv("C:/Users/shane/Downloads/DataWrangling_Final/Credit_Card_Applications.csv")
```

Here we create the Point Biserial to see the columns most useful in prediction. 

From this we can see the most predictive columns are A8, A9, and then A10. Unfortunately the data set does not say what these columns each stand for. Maybe I can reach out to credit card companies and ask for their input and piece it together. 

```{python}
y= df['Class']
x = df.drop(columns = ['Class', 'CustomerID'])

results = {}

for col in x.columns:
    results['correlation_' + str(col)] = pointbiserialr(y, x[col])

pb = pd.DataFrame.from_dict(results, orient='index')
pb = pb.rename(columns = {'statistic': 'r', 'pvalue': 'p'})

pb.index = pb.index.str.replace('correlation_', '', regex = False).str.strip()

pb = pb.dropna(subset = ['r'])

topN = 3

pb_sorted = pb.reindex(pb['r'].abs().sort_values(ascending = False).index)

plt.figure(figsize=(8, 10))
pb_sorted['r'].head(topN).iloc[::-1].plot(kind='barh')
plt.xlabel('Point-biserial r (Accept = 1)')
plt.title(f'Top {topN} Features by |r|')
plt.tight_layout()
plt.show()
```

We now make the Class column into two categories (Denied and Accepted), as it makes the future boxplots look much better!

```{python}
df['Class'] = df['Class'].astype(str).str.strip().map({"0": 'Denied', "1": 'Accepted'})
```

According to the point biserial r the three least important columns are A10, A9, and A8. Now we will create plots for comparisons between these groups in accepted or not. On the Y axis we have the top 3 most impactful columns. On the X axis we have the categories, this allows for a quick comparison between groups. 

```{python}

A10 = (
ggplot(df, aes(y="A10", x = 'Class', fill = 'Class'))
+ geom_boxplot()
+ theme_bw()
+ theme(
panel_grid_major=element_blank(),
panel_grid_minor=element_blank(),
panel_border=element_blank(),
panel_background=element_blank()
)
+ labs(
x="Class", # Axis label
title="A10 Value" # Plot title
)
)

A9 = (
ggplot(df, aes(y="A9", x = 'Class', fill = 'Class'))
+ geom_boxplot()
+ theme_bw()
+ theme(
panel_grid_major=element_blank(),
panel_grid_minor=element_blank(),
panel_border=element_blank(),
panel_background=element_blank()
)
+ labs(
x="Class", # Axis label
title="A9 Value" # Plot title
)
)

A8 = (
ggplot(df, aes(y="A8", x = 'Class', fill = 'Class'))
+ geom_boxplot()
+ theme_bw()
+ theme(
panel_grid_major=element_blank(),
panel_grid_minor=element_blank(),
panel_border=element_blank(),
panel_background=element_blank()
)
+ labs(
x="Class", # Axis label
title="A8 Value" # Plot title
)
)
```



A8 boxplot. 

Here we can see drastically different box plots for accepted or denied cards, and the spread of the A8 values. Unfortunately A8 is an integer, or maybe even a binary. We can see though the majority of variables of A8 that were accepted were a 1, while the majority of denied were a 0. Both have flat box plots meaning the IQR is entirely that number, tehn both have outliers further out. This backs the point biserial, having a higher A8 (or a yes) is very important to getting a credit card!
```{python}
A8
```

A9 boxplot

A9 is very similar to A8, as in it is either a binary or an integer, and only has values at 0 or 1. For the accepted group, the bottom quartile is at 0, while the mediana nd upper quartile are at 1
```{python}
A9
```

A10 boxplot


```{python}
A10
```


Return the Class column back to 0 or 1, so we can run the point biserial. 
```{python}
df['Class'] = df['Class'].map({'Denied': 0, 'Accepted': 1}).astype(int)
```

Now we will look at the 3 least important variables. 

```{python}
results2 = {}

for col in x.columns:
    results2['correlation_' + str(col)] = pointbiserialr(y, x[col])

pb2 = pd.DataFrame.from_dict(results, orient='index')
pb2 = pb2.rename(columns = {'statistic': 'r', 'pvalue': 'p'})

pb2.index = pb2.index.str.replace('correlation_', '', regex = False).str.strip()

pb2 = pb2.dropna(subset = ['r'])

bottomN = 3

pb2_sorted = pb2.reindex(pb['r'].abs().sort_values(ascending = True).index)

plt.figure(figsize=(8, 10))
pb2_sorted['r'].head(bottomN).iloc[::-1].plot(kind='barh')
plt.xlabel('Point-biserial r (Accept = 1)')
plt.title(f'Bottom {bottomN} Features by |r|')
plt.tight_layout()
plt.show()
```


We now make the Class column into two categories (again), as it makes the boxplots look much better!

```{python}
df['Class'] = df['Class'].astype(str).str.strip().map({"0": 'Denied', "1": 'Accepted'})
```

According to the point biserial r the three least important columns are A1, A11, and A13. Now we will create plots for comparisons between these groups in accepted or not. On the Y axis we have the top 3 least impactful columns. On the X axis we have the categories, this allows for a quick comparison between groups. 

```{python}
A1 = (
ggplot(df, aes(y="A1", x = 'Class', fill = 'Class'))
+ geom_boxplot()
+ theme_bw()
+ theme(
panel_grid_major=element_blank(),
panel_grid_minor=element_blank(),
panel_border=element_blank(),
panel_background=element_blank()
)
+ labs(
x="Class", # Axis label
title="A1 Value" # Plot title
)
)

A11 = (
ggplot(df, aes(y="A11", x = 'Class', fill = 'Class'))
+ geom_boxplot()
+ theme_bw()
+ theme(
panel_grid_major=element_blank(),
panel_grid_minor=element_blank(),
panel_border=element_blank(),
panel_background=element_blank()
)
+ labs(
x="Class", # Axis label
title="A11 Value" # Plot title
)
)

A13 = (
ggplot(df, aes(y="A13", x = 'Class', fill = 'Class'))
+ geom_boxplot()
+ theme_bw()
+ theme(
panel_grid_major=element_blank(),
panel_grid_minor=element_blank(),
panel_border=element_blank(),
panel_background=element_blank()
)
+ labs(
x="Class", # Axis label
title="A13 Value" # Plot title
)
)
```

A1 boxplot
```{python}
A1
```

A11 boxplot
```{python}
A11
```

A13 boxplot
```{python}
A13
```


Logistic Regression of Card Accpetance or Denial Data with Lasso

Return the Class column back to 0 or 1. 
```{python}
df['Class'] = df['Class'].map({'Denied': 0, 'Accepted': 1}).astype(int)
```




```{python}
y = df["Class"].to_numpy()
X = df.drop(columns=["CustomerID", 'Class'])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
features = X.columns

x_data = pd.DataFrame(X_scaled, columns=features)

lambda_seq = np.arange(0.1, 10.0 + 1e-12, 0.1)
cv_model = LassoCV(alphas=lambda_seq, cv=10, fit_intercept=True, max_iter=10000)
cv_model.fit(X_scaled, y)
alpha_min = cv_model.alpha_
lasso = Lasso(alpha=alpha_min, fit_intercept=True, max_iter=10000)
lasso.fit(X_scaled, y)

coef_series = pd.Series(lasso.coef_, index=features, name="coef")
coef_df = coef_series.reset_index()

selected_features = coef_series[coef_series != 0].index.tolist()

predictors = [f'Q("{c}")' for c in selected_features]
formula = 'Q("Class") ~ ' + " + ".join(predictors)

fit_3 = smf.logit(formula = formula, data = df).fit()

print(fit_3.summary())

```

This may seem confusing, as a new variable A5, replaces A10. This is most likely due to A10 having a high correaltion with a differetn variable(s) such as A9 and/or A8, whihc means its effect on Class is mostly covered by teh other variables. This means the lasso would remove it, while keeping variables that do not, like A5. Also in the point biserial r, A5 is the 4th highest r value vairable, so it may not have even moved up much to replace A10. 

Lets now test and see how good of a predictor our new logistic regression is, usign the last 25% of the data, or the test data.

Split Data, as I want to see if the logistic regression is a good indicator. 
```{python}
df['Class'] = df['Class'].astype('int')
percent = (len(df)*.75)

train = df.loc[:percent]
test = df.loc[percent:]
```


Now we test usign the tarin and test data above!
```{python}
y_test = test["Class"].to_numpy()
# X_test = test.drop(columns=["CustomerID", 'Class'])

y = train["Class"].to_numpy()
X = train.drop(columns=["CustomerID", 'Class'])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
features = X.columns

x_data = pd.DataFrame(X_scaled, columns=features)

lambda_seq = np.arange(0.1, 10.0 + 1e-12, 0.1)
cv_model = LassoCV(alphas=lambda_seq, cv=10, fit_intercept=True, max_iter=10000)
cv_model.fit(X_scaled, y)
alpha_min = cv_model.alpha_
lasso = Lasso(alpha=alpha_min, fit_intercept=True, max_iter=10000)
lasso.fit(X_scaled, y)

coef_series = pd.Series(lasso.coef_, index=features, name="coef")
coef_df = coef_series.reset_index()

selected_features = coef_series[coef_series != 0].index.tolist()

predictors = [f'Q("{c}")' for c in selected_features]
formula = 'Q("Class") ~ ' + " + ".join(predictors)

fit_3 = smf.logit(formula = formula, data = train).fit()

print(fit_3.summary())


```

Now we can get the accuracy from the above regression. 

```{python}
logit_pred_prob = fit_3.predict(test[selected_features])
logit_pred = (logit_pred_prob >= 0.5).astype(int)
logit_acc = accuracy_score(y_test, logit_pred)

print(f"\nLogistic Regression Accuracy: {logit_acc:.4f}")
```
